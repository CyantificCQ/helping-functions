{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "\n",
    "from datetime import date\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "# dd/mm/YY\n",
    "d1 = today.strftime(\"%d/%m/%Y\")\n",
    "print(\"d1 =\", d1)\n",
    "\n",
    "# Textual month, day and year\t\n",
    "d2 = today.strftime(\"%B %d, %Y\")\n",
    "print(\"d2 =\", d2)\n",
    "\n",
    "# mm/dd/y\n",
    "d3 = today.strftime(\"%m/%d/%y\")\n",
    "print(\"d3 =\", d3)\n",
    "\n",
    "# Month abbreviation, day and year\t\n",
    "d4 = today.strftime(\"%b-%d-%Y\")\n",
    "print(\"d4 =\", d4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "\n",
    "# now = date.today()\n",
    "# now\n",
    "\n",
    "# today = now.strftime(\"%m/%d/%Y\")\n",
    "# today"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Creating weekend days with Nan-s\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "df = df.reindex(pd.date_range(df.index.min(), df.index.max())).sort_index(ascending=False).reset_index().rename(columns={'index': 'Date'})\n",
    "\n",
    "#or\n",
    "\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "df = df.asfreq('d').sort_index(ascending=False).reset_index().rename(columns={'index': 'Date'})\n",
    "\n",
    "#replacing empty cells with NANs\n",
    "df[df[0]==\"\"] = np.NaN\n",
    "\n",
    "#Then use fillna \n",
    "df.fillna(method='ffill')\n",
    "\n",
    "#or using a function\n",
    "def same_as_upper(col:pd.Series)-> pd.Series:\n",
    "    '''\n",
    "    Recursively fill NaN rows with the previous value\n",
    "    '''\n",
    "    if any(pd.Series(col).isna()):\n",
    "        col=pd.Series(np.where(col.isna(), col.shift(1), col))\n",
    "        return same_as_upper(col)\n",
    "    else:\n",
    "        return col\n",
    "\n",
    "\n",
    "#then applying the function\n",
    "df['A']=same_as_upper(df['A'])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "\n",
    "import csv \n",
    "from datetime import datetime\n",
    "\n",
    "time = []\n",
    "gold_price = []\n",
    "\n",
    "with open(\"Gold_data_reversed_filled.csv\", \"r\") as f:\n",
    "    csv_reader = csv.reader(f, delimiter=\",\")\n",
    "    next(csv_reader)\n",
    "    for i in csv_reader:\n",
    "        time.append(datetime.strptime(i[1], \"%Y-%m-%d\" ))\n",
    "        gold_price.append(float(i[2]))\n",
    "        \n",
    "#check if I did right\n",
    "time[:5], gold_price[:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "path  =\"forecasting_csv/\"\n",
    "most_recent_file = None\n",
    "most_recent_time = 0\n",
    "for entry in os.scandir(path):\n",
    "    if entry.is_file():\n",
    "        # get the modification time of the file using entry.stat().st_mtime_ns\n",
    "        mod_time = entry.stat().st_mtime_ns\n",
    "        if mod_time > most_recent_time:\n",
    "            # update the most recent file and its modification time\n",
    "            most_recent_file = entry.name\n",
    "            most_recent_time = mod_time\n",
    "most_recent_file, most_recent_time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "from time import sleep\n",
    "from tqdm import tqdm\n",
    "for i in tqdm(range(10)):\n",
    "    sleep(3)\n",
    "\n",
    " 60%|██████    | 6/10 [00:18<00:12,  0.33 it/s]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python \n",
    "\n",
    "# Set the right index order\n",
    "\n",
    "new_df.index = new_df.index.values[::-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "df2 = pd.concat([df,missing_values])\n",
    "df2\n",
    "\n",
    "# pd.append will be deprecated"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "#Get the index of the last time the old dataframe has\n",
    "\n",
    "index_num = new_df[new_df[\"Date\"]== \"2023-04-25\"].index.item()\n",
    "index_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'glob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mGold_data_reversed_filled.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 6\u001b[0m list_of_files \u001b[39m=\u001b[39m glob\u001b[39m.\u001b[39mglob(\u001b[39m\"\u001b[39m\u001b[39mforecasting_csv/*\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[39m# checking the last file I downloaded in the map\u001b[39;00m\n\u001b[0;32m      8\u001b[0m latest_file \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(list_of_files, key\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mgetmtime)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'glob' is not defined"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time \n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "df = pd.read_csv(\"Gold_data_reversed_filled.csv\")\n",
    "list_of_files = glob.glob(\"forecasting_csv/*\")\n",
    "# checking the last file I downloaded in the map\n",
    "latest_file = max(list_of_files, key=os.path.getmtime)\n",
    "last_date_in_old_df = df.index[-1]\n",
    "\n",
    "new_df = pd.read_csv(latest_file)\n",
    "new_df[\"Date\"] = pd.to_datetime(new_df[\"Date\"])\n",
    "new_df = new_df.iloc[::-1].reset_index()\n",
    "new_df.drop(columns=[\"index\"])\n",
    "new_df = new_df.drop(columns=[\"index\"])\n",
    "df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "# check the index of the last date and creating another dataframe that will be added to the old gold dataframe\n",
    "index_num = new_df[new_df[\"Date\"]== \"2023-04-25\"].index.item()\n",
    "missing_values = pd.DataFrame(new_df.iloc[index_num:])\n",
    "df = df.append(missing_values)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "new_all_gold = pd.DataFrame(df)\n",
    "new_all_gold = new_all_gold.set_index(\"Date\").rename(columns={\"Close/Last\": \"Price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking dates for the function\n",
    "from datetime import datetime \n",
    "\n",
    "\n",
    "old_date = gold_data.index[-1]\n",
    "\n",
    "now = datetime.utcnow()\n",
    "now, old_date\n",
    "\n",
    "days = now- old_date\n",
    "\n",
    "days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gd = pd.read_csv(\"Gold_data_reversed_filled.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_date = gd.Date.iloc[-1]\n",
    "last_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def predicting_on_newgoldprices():\n",
    "    new = glob.glob(\"forecasting_csv/*\")\n",
    "    latest_file = max(new, key=os.path.getmtime)\n",
    "    data = pd.read_csv(latest_file)\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    data.set_index('Date', inplace=True)\n",
    "    data = data.reindex(pd.date_range(data.index.min(), data.index.max())).sort_index(ascending=True).reset_index().rename(columns={'index': 'Date'})\n",
    "    data = data.fillna(method=\"ffill\")\n",
    "    gold_data = data[['Date','Close/Last']].copy()\n",
    "    gold_data = gold_data.rename(columns={\"Close/Last\": \"Price\"})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "You can work out the columns that are only in one DataFrame and use this to select a subset of columns in the merge.\n",
    "\n",
    "```python\n",
    "cols_to_use = df2.columns.difference(df.columns)\n",
    "Then perform the merge (note this is an index object but it has a handy tolist() method).\n",
    "\n",
    "dfNew = merge(df, df2[cols_to_use], left_index=True, right_index=True, how='outer')\n",
    "\n",
    "\n",
    "df = df1.drop_duplicates().merge(df2.drop_duplicates(), on=df2.columns.to_list(), \n",
    "                   how='left', indicator=True)\n",
    "df.loc[df._merge=='left_only',df.columns!='_merge']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv(\"forecasting_csv\\HistoricalData_1685021900505.csv\")\n",
    "# data['Date'] = pd.to_datetime(data['Date'])\n",
    "# data.set_index('Date', inplace=True)\n",
    "# data = data.reindex(pd.date_range(data.index.min(), data.index.max())).sort_index(ascending=True).reset_index().rename(columns={'index': 'Date'})\n",
    "# data = data.fillna(method=\"ffill\")\n",
    "# g_data = data[['Date','Close/Last']].copy()\n",
    "# g_data = g_data.rename(columns={\"Close/Last\": \"Price\"})\n",
    "# g_data = g_data.set_index(\"Date\")\n",
    "# x_test = g_data.iloc[:3645]\n",
    "# y_test = g_data.iloc[3646]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adding_new_data_to_goldcsv():\n",
    "    df = pd.read_csv(\"Gold_data_reversed_filled.csv\")\n",
    "    list_of_files = glob.glob(\"forecasting_csv/*\")\n",
    "    latest_file = max(list_of_files, key=os.path.getmtime)\n",
    "    last_date_in_old_df = df.Date.iloc[-1]\n",
    "    new_df = pd.read_csv(latest_file)\n",
    "    last_date_in_new_df = new_df.Date.iloc[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time \n",
    "\n",
    "df = pd.read_csv(\"Gold_data_reversed_filled.csv\")\n",
    "list_of_files = glob.glob(\"forecasting_csv/*\")\n",
    "# checking the last file I downloaded in the map\n",
    "latest_file = max(list_of_files, key=os.path.getmtime)\n",
    "last_date_in_old_df = df.index[-1]\n",
    "\n",
    "new_df = pd.read_csv(latest_file)\n",
    "new_df[\"Date\"] = pd.to_datetime(new_df[\"Date\"])\n",
    "new_df = new_df.iloc[::-1].reset_index()\n",
    "new_df.drop(columns=[\"index\"])\n",
    "new_df = new_df.drop(columns=[\"index\"])\n",
    "df = df.drop(columns=[\"Unnamed: 0\"])\n",
    "# check the index of the last date and creating another dataframe that will be added to the old gold dataframe\n",
    "index_num = new_df[new_df[\"Date\"]== \"2023-04-25\"].index.item()\n",
    "missing_values = pd.DataFrame(new_df.iloc[index_num:])\n",
    "df = df.append(missing_values)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "new_all_gold = pd.DataFrame(df)\n",
    "new_all_gold = new_all_gold.set_index(\"Date\").rename(columns={\"Close/Last\": \"Price\"})\n",
    "\n",
    "\n",
    "# Need to delete the old csv file and save the other with the new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.firefox import firefox_binary\n",
    "from selenium.webdriver import Firefox, FirefoxOptions\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "def fetch():\n",
    "    print(\"Historical data downloading please wait\")\n",
    "    for i in range(5):\n",
    "        string = \". \" * i\n",
    "        print(string, end=\"\\r\")\n",
    "        time.sleep(2.5)\n",
    "\n",
    "def downloading_historical_data():    \n",
    "    import time\n",
    "    fetch()\n",
    "    options = Options()\n",
    "    # options = Options()\n",
    "    # options.add_argument(\"start-maximized\")\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    # options.add_argument(\"--window_size=1920,1080\")\n",
    "    # options.add_argument(\"--start-maximized\")\n",
    "    # options.add_argument('--disable-gpu')\n",
    "    # options.add_argument('--no-sandbox')\n",
    "    # options.add_argument(\"--disable-extensions\")\n",
    "    # options.add_argument('disable-infobars')\n",
    "    options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "    options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "    options.add_experimental_option(\"prefs\", {\"download.default_directory\": r\"C:\\Users\\stadl\\MACHINE_LEARNING_PRACTICE\\Saját\\Stock_Forecasting\\forecasting_csv\"})\n",
    "    # for x in range (4):\n",
    "    #     string = \"Downloading new historical data please wait\" + \".\" * x\n",
    "    #     print(\"\\033[K\", string, end=\"\\r\") \n",
    "    # Define the URL of the webpage\n",
    "    url = \"https://www.nasdaq.com/market-activity/commodities/gc:cmx/historical\"\n",
    "\n",
    "    # Set up the Selenium webdriver (you may need to specify the path to your chromedriver executable)\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH,'//button[@id=\"onetrust-accept-btn-handler\"]'))).click()\n",
    "    # Click the \"Download to Spreadsheet\" button\n",
    "    driver.find_element(By.XPATH, \"//button[@aria-label='Click to show maximum available data']\").click()\n",
    "    driver.implicitly_wait(5)\n",
    "    driver.find_element(By.XPATH, \"//button[@class='historical-data__controls-button--download historical-download']\").click()\n",
    "    # clear the line, print string and go back to the start\n",
    "    # button.click()\n",
    "    driver.implicitly_wait(20)\n",
    "\n",
    "    # Wait for the download to complete (adjust the sleep time as necessary)\n",
    "    import time\n",
    "    time.sleep(10)\n",
    "    driver.quit()\n",
    "    # Read the downloaded CSV file into a pandas DataFrame\n",
    "    # Print the first few rows of the DataFrame\n",
    "    # Close the webdriver\n",
    "    # string2 = \"\\nThe new historical data downloaded\"\n",
    "    print(\"\\nThe new historical data downloaded\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preparing_data():\n",
    "#     downloading_historical_data()\n",
    "#     new = glob.glob(\"forecasting_csv/*\")\n",
    "#     latest_file = max(new, key=os.path.getmtime)\n",
    "#     data = pd.read_csv(latest_file)\n",
    "#     data['Date'] = pd.to_datetime(data['Date'])\n",
    "#     data.set_index('Date', inplace=True)\n",
    "#     data = data.reindex(pd.date_range(data.index.min(), data.index.max())).sort_index(ascending=True).reset_index().rename(columns={'index': 'Date'})\n",
    "#     data = data.fillna(method=\"ffill\")\n",
    "#     g_data = data[['Date','Close/Last']].copy()\n",
    "#     g_data = g_data.rename(columns={\"Close/Last\": \"Price\"})\n",
    "#     g_dta = g_data.set_index(\"Date\")\n",
    "#     g_data = g_data.iloc[:3645]\n",
    "#     return g_data, data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arima, Prophet, Neural Prophet, Vector Autoregression (Var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
